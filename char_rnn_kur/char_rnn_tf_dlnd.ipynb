{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "[dataset link](https://github.com/udacity/deep-learning/tree/master/intro-to-rnns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T03:21:04.715212Z",
     "start_time": "2017-03-09T11:21:02.932537+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T03:21:05.065377Z",
     "start_time": "2017-03-09T11:21:04.717315+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T03:21:05.087632Z",
     "start_time": "2017-03-09T11:21:05.068065+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T03:21:06.733333Z",
     "start_time": "2017-03-09T11:21:06.726384+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T06:30:12.587633Z",
     "start_time": "2017-03-09T14:30:12.569177+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985223"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)# all unique characters including symbols and punct # how to access individual element of a set?\n",
    "type(vocab_to_int) # all unique characters paired with index # how to access index and values of a dictionary?\n",
    "int_to_vocab # index paired with characters\n",
    "len(chars) # total number of characters of the whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T06:30:13.252255Z",
     "start_time": "2017-03-09T14:30:13.242947+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 50, 73,  0, 42,  4,  3, 40,  6, 71, 71, 71, 26, 73,  0,  0, 23,\n",
       "       40,  8, 73, 39, 55, 46, 55,  4, 74, 40, 73,  3,  4, 40, 73, 46, 46,\n",
       "       40, 73, 46, 55,  2,  4,  1, 40,  4, 41,  4,  3, 23, 40, 47,  7, 50,\n",
       "       73,  0,  0, 23, 40,  8, 73, 39, 55, 46, 23, 40, 55, 74, 40, 47,  7,\n",
       "       50, 73,  0,  0, 23, 40, 55,  7, 40, 55, 42, 74, 40,  9, 81,  7, 71,\n",
       "       81, 73, 23, 10, 71, 71, 54, 41,  4,  3, 23, 42, 50, 55,  7], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Making training and validation batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T06:16:38.739614Z",
     "start_time": "2017-03-09T14:16:38.730254+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2, 3]), array([4, 5]), array([6, 7])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(np.array([2,3,4,5,6,7]), 3)#.shape\n",
    "# np.stack(np.split(np.array([2,3,4,5,6,7]), 3))\n",
    "# np.stack(np.split(np.array([2,3,4,5,6,7]), 3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T06:29:42.903587Z",
     "start_time": "2017-03-09T14:29:42.884855+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "                                            # 1 character * num_steps(meaning: how many chars as 1 sample) = 1 sample \n",
    "                                            # 1 sample * batch_size(meaning: how many samples as 1 batch)= 1 batch = 1 slice_size \n",
    "    slice_size = batch_size * num_steps\n",
    "    \n",
    "                                            # 1 batch or 1 slice_size * num_batches = total chars = a whole text\n",
    "                                            # int(6.02) == 6, drop a little rest\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "                                            # Drop the last few characters to make only full batches\n",
    "                                            # y is shift 1 char right to x\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    \n",
    "                                            # np.split(np.array([2,3,4,5,6,7]), 3) = split into a list of 3 arrays, \n",
    "                                            # each array has 2 elements\n",
    "                                            # np.stack(): turn list into 2-d array (3,2)\n",
    "                                            # now: x is 2-d array dim = (batch_size, x/batch_size) \n",
    "                                            # now: y is 2-d array dim = (batch_size, y/batch_size) \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "                                            # split into train and valid sets: split_frac is train, rest is valid\n",
    "                                            # split not by rows, but split on columns, 10 rows remains 10 rows = batch_size\n",
    "                                            # total chars = 1 char * num_steps * batch_size * num_batches, since batch_size stays\n",
    "                                            # num_train_columns = split_frac * (num_batches * num_steps)\n",
    "                                            # int() to remove the little rest\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T06:30:40.140420Z",
     "start_time": "2017-03-09T14:30:40.129843+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T06:39:01.024811Z",
     "start_time": "2017-03-09T14:39:01.016687+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 178650)\n",
      "(10, 178650)\n",
      "(10, 19850)\n",
      "(10, 19850)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "**Check 1st batch**: \n",
    "- 1 batch = batch_size * 1 sample = batch_size * (1 char * num_step)\n",
    "- batch_size = 10\n",
    "- num_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T06:41:14.026810Z",
     "start_time": "2017-03-09T14:41:13.997218+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 50, 73,  0, 42,  4,  3, 40,  6, 71, 71, 71, 26, 73,  0,  0, 23,\n",
       "        40,  8, 73, 39, 55, 46, 55,  4, 74, 40, 73,  3,  4, 40, 73, 46, 46,\n",
       "        40, 73, 46, 55,  2,  4,  1, 40,  4, 41,  4,  3, 23, 40, 47,  7],\n",
       "       [40, 73, 39, 40,  7,  9, 42, 40, 11,  9, 55,  7, 11, 40, 42,  9, 40,\n",
       "        74, 42, 73, 23, 52, 65, 40, 73,  7, 74, 81,  4,  3,  4, 13, 40, 43,\n",
       "         7,  7, 73, 52, 40, 74, 39, 55, 46, 55,  7, 11, 52, 40, 75, 47],\n",
       "       [41, 55,  7, 10, 71, 71, 65, 68,  4, 74, 52, 40, 55, 42, 48, 74, 40,\n",
       "        74,  4, 42, 42, 46,  4, 13, 10, 40, 17, 50,  4, 40,  0,  3, 55, 12,\n",
       "         4, 40, 55, 74, 40, 39, 73, 11,  7, 55,  8, 55, 12,  4,  7, 42],\n",
       "       [ 7, 40, 13, 47,  3, 55,  7, 11, 40, 50, 55, 74, 40, 12,  9,  7, 41,\n",
       "         4,  3, 74, 73, 42, 55,  9,  7, 40, 81, 55, 42, 50, 40, 50, 55, 74,\n",
       "        71, 75,  3,  9, 42, 50,  4,  3, 40, 81, 73, 74, 40, 42, 50, 55],\n",
       "       [40, 55, 42, 40, 55, 74, 52, 40, 74, 55,  3, 53, 65, 40, 74, 73, 55,\n",
       "        13, 40, 42, 50,  4, 40,  9, 46, 13, 40, 39, 73,  7, 52, 40, 11,  4,\n",
       "        42, 42, 55,  7, 11, 40, 47,  0, 52, 40, 73,  7, 13, 71, 12,  3],\n",
       "       [40,  5, 42, 40, 81, 73, 74, 71,  9,  7, 46, 23, 40, 81, 50,  4,  7,\n",
       "        40, 42, 50,  4, 40, 74, 73, 39,  4, 40,  4, 41,  4,  7, 55,  7, 11,\n",
       "        40, 50,  4, 40, 12, 73, 39,  4, 40, 42,  9, 40, 42, 50,  4, 55],\n",
       "       [50,  4,  7, 40, 12,  9, 39,  4, 40,  8,  9,  3, 40, 39,  4, 52, 65,\n",
       "        40, 74, 50,  4, 40, 74, 73, 55, 13, 52, 40, 73,  7, 13, 40, 81,  4,\n",
       "         7, 42, 40, 75, 73, 12,  2, 40, 55,  7, 42,  9, 40, 42, 50,  4],\n",
       "       [ 1, 40, 75, 47, 42, 40,  7,  9, 81, 40, 74, 50,  4, 40, 81,  9, 47,\n",
       "        46, 13, 40,  3,  4, 73, 13, 55, 46, 23, 40, 50, 73, 41,  4, 40, 74,\n",
       "        73, 12,  3, 55,  8, 55, 12,  4, 13, 52, 40,  7,  9, 42, 40, 39],\n",
       "       [42, 40, 55, 74,  7, 48, 42, 10, 40, 17, 50,  4, 23, 48,  3,  4, 40,\n",
       "         0,  3,  9,  0,  3, 55,  4, 42,  9,  3, 74, 40,  9,  8, 40, 73, 40,\n",
       "        74,  9,  3, 42, 52, 71, 75, 47, 42, 40, 81,  4, 48,  3,  4, 40],\n",
       "       [40, 74, 73, 55, 13, 40, 42,  9, 40, 50,  4,  3, 74,  4, 46,  8, 52,\n",
       "        40, 73,  7, 13, 40, 75,  4, 11, 73,  7, 40, 73, 11, 73, 55,  7, 40,\n",
       "         8,  3,  9, 39, 40, 42, 50,  4, 40, 75,  4, 11, 55,  7,  7, 55]], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "\n",
    "**split train_x, train_y into batches**: \n",
    "- arrs: [train_x, train_y] or [val_x, val_y]\n",
    "- slice_size is a misleading name: its actual identity == x/batch_size == num_steps * n_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T07:03:55.389493Z",
     "start_time": "2017-03-09T15:03:55.377433+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "                                        # - arrs == [train_x, train_y] or [val_x, val_y]\n",
    "def get_batch(arrs, num_steps):\n",
    "    \n",
    "                                        # - slice_size is a misleading name: its actual identity == x/batch_size\n",
    "                                                                                               # == num_steps * n_batches\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    \n",
    "                                        # loop each batch\n",
    "    for b in range(n_batches):\n",
    "                                        # loop between [train_x, train_y]\n",
    "                                        # train_x, for 1st batch, keep all rows (10 rows total), select columns from 0 to 49\n",
    "                                        # train_y, for 1st batch, keep all rows (10 rows total), select columns from 0 to 49\n",
    "                                        # train_x, for 2nd batch, keep all rows (10 rows total), select columns from 50 to 99\n",
    "                                        # train_y, for 2nd batch, keep all rows (10 rows total), select columns from 50 to 99\n",
    "                                        # yield: is to put all above into an object iterable\n",
    "                    # https://hyp.is/dJ-VvgSWEeeGA_P8j321Fg/pythontips.com/2013/09/29/the-python-yield-keyword-explained/\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]\n",
    "        \n",
    "                                        # yield help split train_x into n_batches of batches, each with dim (10, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "                                                # set sampling = True, this way, we only feed 1 char to model at a time\n",
    "                                                # this can speed up \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "        \n",
    "        \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "                                                # Declare placeholders for inputs and targets\n",
    "                                                # inputs and targets are tensors, each with dim (batch_size, num_steps)\n",
    "                                                # there are n_batches of these tensors in total\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    \n",
    "                                                # define probability placeholder to feed in dropout value\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    \n",
    "                                                # One-hot encoding the input and target tensors\n",
    "                                                # now tensor is 3-d (batch_size, num_steps, vocab)\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    \n",
    "    \n",
    "                                                ### Build the RNN layers\n",
    "                                                # Use a basic LSTM cell with 128 neurons\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    \n",
    "                                                # Add dropout to the cell, with dropout value placehoder added\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    \n",
    "                                                # Stack up multiple LSTM layers, here are 2 layers required\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "                                                # initialize the state with zero state\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "                                                ### turn x_one_hot 3-d into rnn_inputs as 1-d???????\n",
    "                                                \n",
    "                                                # tf.split: split x_one_hot (batch_size, num_steps, vocab) into \n",
    "                                                # num_steps of 2-d tenors, each with dim (batch_size, vocab)\n",
    "                                                # tf.squeeze(tensor, axis=[1]), to squeeze off second dim\n",
    "                                                # now we have a list of num_steps of 1-d tensor with dim (batch_size*vocab, )\n",
    "                            \n",
    "                            # https://hyp.is/Gt7csASbEeeNUk9ozR_ZuA/www.tensorflow.org/api_docs/python/tf/split\n",
    "                            # https://hyp.is/Pmiw-ASaEeeMJkOMBxWpOw/www.tensorflow.org/api_docs/python/tf/squeeze\n",
    "    rnn_inputs = [tf.squeeze(i, axis=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "                                                # now rnn_inputs is a list of num_steps of column vectors\n",
    "    \n",
    "    \n",
    "                                                # Run each sequence step (num_steps in total) through the RNN and \n",
    "                                                # collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    \n",
    "                                                # outputs are outputs of all hidden layers,\n",
    "                                                # tf.concat to join them together into one large output\n",
    "                                                # tf.reshape into a large output tensor dim (-1, lstm_size)\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    \n",
    "    \n",
    "                                                ### build a softmax layer\n",
    "                                                # build softmax_w and softmax_b variable holders\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "        \n",
    "    \n",
    "                                                # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "                                                # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "                                                # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    \n",
    "                                                # Reshape the targets to match the logits for calc cross-entropy\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    \n",
    "                                                # calc loss with cross-entropy\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    \n",
    "    \n",
    "                                                # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Export the nodes using namedtuple\n",
    "    # https://hyp.is/3dAyfgSfEee9XvuDFXgZZA/pymotw.com/2/collections/namedtuple.html\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    \n",
    "                                                # use to use locals()\n",
    "# https://hyp.is/iQd3kgSgEeei6T9SRPMc5A/stackoverflow.com/questions/7969949/whats-the-difference-between-globals-locals-and-vars\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that heps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4176 9.2723 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3724 6.3679 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.1883 6.5060 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.4842 6.6540 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.4509 6.3942 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.3708 6.3062 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.2867 6.2887 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.2082 6.3095 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.1324 6.2057 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 4.0661 6.7661 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 4.0063 7.0095 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.9577 6.6022 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.9159 6.7216 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.8795 6.4497 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.8457 6.7921 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.8146 7.0849 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.7850 9.5773 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.7603 9.0856 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.7368 7.4370 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.7131 7.1229 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.6930 6.7948 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.6743 7.0266 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.6563 6.3855 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.6399 6.5743 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.6243 8.0877 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.6102 7.7721 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.5971 13.5447 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.5838 9.2296 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.5715 10.0188 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.5603 6.6987 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.5504 10.4358 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.5398 9.2206 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.5297 7.0134 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.5206 9.7965 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.5115 9.4673 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.5031 7.1072 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4945 7.0831 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4865 7.3095 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.4788 7.9755 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.4713 7.3996 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.4641 6.3258 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.4573 6.2570 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.4507 6.3115 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.4442 7.0647 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.4381 6.3858 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.4323 6.3191 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.4270 8.5994 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.4218 6.6443 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.4168 9.5921 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.4120 7.3277 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-780c3a10820d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                     model.initial_state: new_state}\n\u001b[1;32m     34\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n\u001b[0;32m---> 35\u001b[0;31m                                                  feed_dict=feed)\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Natsume/miniconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/____.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/a69d188f8e607228fca61377e6898cdb"
  },
  "gist": {
   "data": {
    "description": "Anna KaRNNa.ipynb",
    "public": true
   },
   "id": "a69d188f8e607228fca61377e6898cdb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "226px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "305px",
    "left": "1041px",
    "right": "20px",
    "top": "7px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
